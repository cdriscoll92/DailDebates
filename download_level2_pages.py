## 2. Download the first page for each Dail term

def open_file_and_read_html(filename):
    with open(filename) as f:
        page_content = f.read()
        bs_page = BeautifulSoup(page_content, "html.parser")
    return(bs_page)

## 3. Get the number of pages each Dail term has
## Number of pages of results for this Dail term

def term_page_length(bs_page):
    n_pages_bs = bs_page.select("span.c-page-num__ref")[0].contents

    match = re.search(r"(\d+).*?(\d+)", str(n_pages_bs))
    n_pages = match.group(2)
    return(n_pages)

## 4. Get the urls for all pages for each Dail term

## Multiple pages of results per term. Getting the links for each
## page of results. The number of pages per term is generated by term_page_length().

def link_gen_subpages_per_dail(dail_term, n):
    link_front = "https://www.oireachtas.ie/en/debates/find/?page="
    link_middle = "&datePeriod=term&debateType=dail&term=%2Fie%2Foireachtas%2Fhouse%2Fdail%2F"
    link_end = "&resultsPerPage=20"
    subpage_links = []
    for i in range(1, int(n)+1):
        subpage_links.append(link_front + str(i) + link_middle + str(dail_term) + link_end)
    return(subpage_links)


## 5. Download all the pages for each Dail term

def level_2_pages_download(url):
    directory = file_directory + "Level 2 pages/"
    page_num = re.search("page=([0-9]+)", url).group(1)
    dail_term = re.search("dail%2F([0-9]+)", url).group(1)
    file_name = dail_term + "_" + page_num + ".html"
    file_path = directory + file_name

    time.sleep(2)
    page = requests.get(url).text
    with open(file_path, "w+") as file:
        file.write(page)






