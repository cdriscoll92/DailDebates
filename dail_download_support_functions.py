import time
import re
import json
import bs4
from bs4 import BeautifulSoup
import requests

## 2. Download the first page for each Dail term

def open_file_and_read_html(filename):
    with open(filename) as f:
        page_content = f.read()
        bs_page = BeautifulSoup(page_content, "html.parser")
    return(bs_page)

## 3. Get the number of pages each Dail term has
## Number of pages of results for this Dail term

def term_page_length(bs_page):
    n_pages_bs = bs_page.select("span.c-page-num__ref")[0].contents

    match = re.search(r"(\d+).*?(\d+)", str(n_pages_bs))
    n_pages = match.group(2)
    return(n_pages)

## 4. Get the urls for all pages for each Dail term

## Multiple pages of results per term. Getting the links for each
## page of results. The number of pages per term is generated by term_page_length().

def link_gen_subpages_per_dail(dail_term, n):
    link_front = "https://www.oireachtas.ie/en/debates/find/?page="
    link_middle = "&datePeriod=term&debateType=dail&term=%2Fie%2Foireachtas%2Fhouse%2Fdail%2F"
    link_end = "&resultsPerPage=20"
    subpage_links = []
    for i in range(1, int(n)+1):
        subpage_links.append(link_front + str(i) + link_middle + str(dail_term) + link_end)
    return(subpage_links)


## 5. Download all the pages for each Dail term

def level_2_pages_download(url):
    directory = file_directory + "Level 2 pages/"
    page_num = re.search("page=([0-9]+)", url).group(1)
    dail_term = re.search("dail%2F([0-9]+)", url).group(1)
    file_name = dail_term + "_" + page_num + ".html"
    file_path = directory + file_name

    time.sleep(2)
    page = requests.get(url).text
    with open(file_path, "w+") as file:
        file.write(page)

def BS_html_parsed_from_html(URL):
    time.sleep(2)
    bs_out = BeautifulSoup(requests.get(URL).text, "html.parser")
    return(bs_out)



def get_debate_links(bs_page):
    debate_links_front = "https://www.oireachtas.ie"
    debate_links_raw = bs_page.findAll("a", {"class": "c-debates-expanding-list__button"})
    debate_links_end = [debate_links_raw[i]['href'] for i in range(len(debate_links_raw))]
    debate_links = [debate_links_front + str(debate_links_end[i]) \
                    for i in range(len(debate_links_end))]
    return(debate_links)

## 5.1 Get date of speeches 
## These are unique by link so best to get them at this step.

def get_date_L_2(URL):
    date = re.search("[0-9]{4}-[0-9]{2}-[0-9]{2}", URL).group()
    return(date)

## 6. Get urls for all speeches in each page

def this_day_debate_links(URL):
    debate_links_front = "https://www.oireachtas.ie"
    this_day_bs_out = BS_html_parsed_from_html(URL)
    this_day_debate_links_list_raw = this_day_bs_out.select("div.results ul li a")
    this_day_debate_links_list = []
    for i in range(len(this_day_debate_links_list_raw)):
        cleaned_debate_links_end = this_day_debate_links_list_raw[i]['href']
        this_day_debate_links_list.append(debate_links_front + cleaned_debate_links_end)
    ## Dropping snippets of pages
    this_day_debate_links_list = [x for x in this_day_debate_links_list if "#" not in x]
    return(this_day_debate_links_list)

## 7. Process speech pages for each page in all pages for each Dail term

def speech_processing_single_debate(single_debate_url):
    this_debate_page = BS_html_parsed_from_html(single_debate_url)
    this_debate_subject = this_debate_page.select("h2")[0].contents[0]
    this_debate_speech_segments = this_debate_page.findAll('div', {'class': 'speech'})
    this_debate_dict = {'subject': this_debate_subject,
                            'speeches': []}
    for i in range(len(this_debate_speech_segments)):
        ## Speaker ID -- only one
        this_debate_speaker_ID = this_debate_speech_segments[i].findAll("a", {"class":"c-avatar__name-link"})
        if this_debate_speaker_ID != []:
            this_debate_segment_speaker = this_debate_speaker_ID[0]
            speaker_ID_cleaned = this_debate_segment_speaker['href'].split("/")[-2]
        else:
            speaker_ID_cleaned = ""
        ## Speaker name -- only one
        this_debate_speaker_name = "".join(this_debate_speech_segments[i].select("h4")[0].text[:])
        ## Speech -- multiple paragraphs
        this_debate_all_speech_segments = this_debate_speech_segments[i].findAll("p")
        speech_joined = ""
        text_to_join = [] ## Speech text
        
        ## Check to see if I can just type this_debate_all_speech_segments[j].contents.text
        for j in range(len(this_debate_all_speech_segments)):
            speech_segment_contents = this_debate_all_speech_segments[j].contents
            ## Emphasized text I still want
            for s in speech_segment_contents:
                if isinstance(s, bs4.element.NavigableString):
                    text_to_join.append(s)
                else:
                    emphasized_text = re.search("<em>",str(s))
                    if emphasized_text is not None:
                        text_to_join.append(s.text[:])
                        
        speech_joined = "\n".join(text_to_join)
#         print(speech_joined)
        speech_joined = re.sub("([a-z, ])\n([A-Za-z0-9 ])", "\\1 \\2", speech_joined)
        this_speech_dict = {'ID': speaker_ID_cleaned,
                            'name': this_debate_speaker_name,
                            'text': speech_joined}
        this_debate_dict['speeches'].append(this_speech_dict)
    return(this_debate_dict)


def speech_processing_whole_day(this_day_url, debates_folder):
    # print(this_day_url)
    link_list = this_day_debate_links(this_day_url)
    this_day_bs_obj = BS_html_parsed_from_html(this_day_url)
    dail_str = this_day_bs_obj.findAll('title')[0].text
    this_dail = re.search("[0-9]+", dail_str).group()
    this_date = get_date_L_2(this_day_url)
    
    this_days_debates_list = []
    for link in link_list:
        # print(link)
        single_debate_dict = speech_processing_single_debate(link)
        this_days_debates_list.append(single_debate_dict)
    
    this_day_debates_dict = {'dail': this_dail,
                             'url': this_day_url,
                             'date': this_date,
                             'debates': this_days_debates_list}
    
    ## Writing to file
    file_name = debates_folder + this_date + ".json"
    with open(file_name, "w+") as f:
        json.dump(this_day_debates_dict, f)



